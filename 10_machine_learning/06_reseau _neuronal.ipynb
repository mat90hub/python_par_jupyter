{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction d'un réseau de neurones.\n",
    "\n",
    "Ce cahier est en majeur partie copié de cette [page](https://www.miximum.fr/blog/introduction-au-deep-learning-1/) du site Miximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction d'un premier réseau\n",
    "\n",
    "Voici le schema d'un neurone, qui fait donc la somme pondérée des entrées et ensuite donne une sortie en passant par un filtre fait par une fonction sigmoïde.\n",
    "\n",
    "![schéma d'un neurone](./img/neurone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous construisons ce neurone sous forme d'une classe. Ce neurone reçoit un certains nombre d'entrées dont il fait une somme pondérée puis un filtrage par la fonction sigmoïde. À son initialisation, les poids donnés à chaque entrée sont aléatoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"Initialisation des poids / biais avec des valeurs aléatoires.\"\"\"\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        self.bias = np.random.randn()\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"On suppose que X est de la taille passée dans le constructeur.\"\"\"\n",
    "        aggregation = np.sum(X * self.weights) + self.bias\n",
    "        return sigmoid(aggregation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les neurones vont être ensuite rassemblés dans une réseau organisé par couches. Ces couches vont qualifier la profondeur du réseau.\n",
    "\n",
    "![réseau de neurones](./img/reseau.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici aussi la classe pour définir chacune de ces couches. Les couches ont deux propriétés: le nombre des neurones dans la couche et le nombre d'entrées arrivant dans chaque neurone de cette couche.\n",
    "\n",
    "La méthode `forward` qui s'applique à cette objet `layer` fait ici appelle à deux méthodes, l'`aggregation` qui est la somme pondérée et l'activation, qui est le filtrage par la fonction sigmoïde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Une seule couche de neurones.\"\"\"\n",
    "    def __init__(self, size, input_size):\n",
    "        self.size = size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Les poids sont représentés par une matrice de n lignes\n",
    "        # et m colonnes. n = le nombre de neurones, m = le nombre de\n",
    "        # neurones dans la couche précédente.\n",
    "        self.weights = np.random.randn(size, input_size)\n",
    "\n",
    "        # Un biais par neurone\n",
    "        self.biases = np.random.randn(size)\n",
    "\n",
    "    # Résultat du calcul de chaque neurone.\n",
    "    # Il est important de noter que `data` est un vecteur (normalement, de\n",
    "    # longueur `self.input_size`, et que nous retournons un vecteur de\n",
    "    # taille `self.size`.\n",
    "    def forward(self, data):\n",
    "        aggregation = self.aggregation(data)\n",
    "        activation = self.activation(aggregation)\n",
    "        return activation\n",
    "\n",
    "    # Calcule la somme des entrées pondérées + biais pour chaque neurone.\n",
    "    # Plutôt que d'utiliser une boucle for, nous tirons parti du calcul\n",
    "    # matriciel qui permet d'effectuer toutes ces opérations d'un coup.\n",
    "    def aggregation(self, data):\n",
    "        # return np.dot(self.weights, data) + self.biases\n",
    "        return self.weights @ data + self.biases  # forme équivalente à np.dot(x,y)\n",
    "\n",
    "    # Passe les valeurs aggrégées dans la moulinette de la fonction\n",
    "    # d'activation.\n",
    "    # `x` est un vecteur de longueur `self.size`, et nous retournons un\n",
    "    # vecteur de même dimension.\n",
    "    def activation(self, x):\n",
    "        return sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le réseau va être ensuite constitué d'une liste de couches. Il y a aussi toujours une dimension pour les entrées. La méthode `add_layer` ajoute couche dans la liste de couche du réseau. La méthode `feedforward` ne fait que de propager cette méthode au travers des couches. La méthode `predict` donne la sortie du réseau et la méthode `evaluate` donne un idée de la performance de la reconnaissance faite par le réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \"\"\"Un réseau constitué de couches de neurones.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, size):\n",
    "        if len(self.layers) > 0:\n",
    "            input_dim = self.layers[-1].size\n",
    "        else:\n",
    "            input_dim = self.input_dim\n",
    "        self.layers.append(Layer(size, input_dim))\n",
    "\n",
    "    # Propage les données d'entrée d'une couche à l'autre.\n",
    "    def feedforward(self, input_data):\n",
    "        activation = input_data\n",
    "        for layer in self.layers:\n",
    "            activation = layer.forward(activation)\n",
    "        return activation\n",
    "\n",
    "    # Retourne l'index du neurone de sortie qui a la plus haute valeur, ce\n",
    "    # qui revient à indiquer quelle classe est sélectionnée par le réseau.\n",
    "    def predict(self, input_data):\n",
    "        return np.argmax(self.feedforward(input_data))\n",
    "\n",
    "    # Évalue la performance du réseau à partir d'un set d'exemples.\n",
    "    # Retourne un nombre entre 0 et 1.\n",
    "    def evaluate(self, X, Y):\n",
    "        results = [1 if self.predict(x) == y else 0 for (x, y) in zip(X, Y)]\n",
    "        accuracy = sum(results) / len(results)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données pour tester le réseau neuronal.\n",
    "\n",
    "L'extension [sklearn.datasets](https://scikit-learn.org/stable/datasets.html) propose différents jeux de données pour exercer des outils de reconnaissance. Parmi eux, nous utiliseront la jeux appelé [digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits). L'objectif pour le réseau neuronal sera de classer ces images selon le chiffre reconnu entre 0 et 9.\n",
    "\n",
    "Les lignes de codes ci-dessous affichent quelques exemples d'images venant de ces donnéeS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAACPCAYAAADEKMPqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPFElEQVR4nO3dfUyV5R/H8c8RAyJB8SkxSxTdLBmS2paaT/Ngmpqk+fCHU5xNKp3Zw4YrTcAyTdt60Mz8R1NzahlkyxSn5Fr/JHosTTdM1GW6fAB8wCfw/v3RPL/wYHAdz+FccN6vrS3uc33v6zp+2flwHzj35XIcxxEAAAi5JqFeAAAA+AehDACAJQhlAAAsQSgDAGAJQhkAAEsQygAAWIJQBgDAEoQyAACWIJQBALBEgwrljIwMJSYm+lWbnZ0tl8sV2AUh4OhxeKDPjR899k9AQtnlctXpv8LCwkBM16j8/PPPeuqppxQTE6N27dpp1qxZunz5cqiX5YMe+2fHjh2aNm2akpOTFRER4feLVH2hz+YqKiq0fPlyDR06VAkJCYqNjdXjjz+uFStWqKqqKtTL80GP/bNw4UI9+eSTatOmjaKjo9W1a1fNnj1bZ8+eDeg8rkDc+3rdunXVvv7iiy9UUFCgtWvXVjuelpamBx980O95bt68qVu3bikqKsq4trKyUpWVlYqOjvZ7/kDzeDzq06ePHn30UU2fPl1//vmnli5dqsGDB2vbtm2hXl419Ng/GRkZ2rhxo3r27KmTJ08qIiJCx48fD/Wy7oo+mzt48KBSUlI0ZMgQDR06VHFxcdq+fbu++eYbTZ48WWvWrAn1Equhx/4ZO3as2rRpo27duik2NlaHDx/WqlWr1LZtW3k8Hj3wwAOBmcgJghkzZjh1OfWVK1eCMX2DMXz4cCchIcEpLy/3Hlu1apUjydm+fXsIV1Y7elw3p06dcm7cuOE4juOMGDHC6dixY2gXZIg+1+7s2bPOwYMHfY5PnTrVkeQUFxeHYFV1R4/999VXXzmSnA0bNgTsnPX2O+VBgwYpOTlZRUVFGjBggGJiYvTmm29KkvLz8zVixAi1b99eUVFRSkpK0oIFC3ze+rnzdxTHjx+Xy+XS0qVL9fnnnyspKUlRUVF64okn9Msvv1Srrel3FC6XSzNnzlReXp6Sk5MVFRWl7t2764cffvBZf2FhoXr37q3o6GglJSVp5cqVNZ7z3LlzOnLkiCoqKv7z3+PixYsqKCjQpEmTFBcX5z0+efJkNWvWTJs2bfrPehvRY1/t27fXfffdV+u4hoQ+V9e6dWt1797d5/hzzz0nSTp8+PB/1tuIHtfN7edXVlbmV31NmgbsTHVw/vx5DR8+XBMnTtSkSZO8b42sXr1azZo102uvvaZmzZpp165devvtt3Xx4kUtWbKk1vN++eWXunTpkjIzM+VyufT+++9rzJgxOnbsWK0viD/99JO2bNmil19+WbGxsfr44481duxYnTx5Uq1atZIk7d+/X8OGDVNCQoJycnJUVVWl3NxctWnTxud8y5YtU05Ojnbv3q1Bgwbddd7ffvtNlZWV6t27d7XjkZGRSk1N1f79+2t93jaix+GBPtfuzJkzkv4J7YaIHvtyHEfnz59XZWWliouLNWfOHEVERAT2dSBg19z/UtPbIQMHDnQkOZ999pnP+IqKCp9jmZmZTkxMjHPt2jXvsSlTplR7+6+kpMSR5LRq1cq5cOGC93h+fr4jydm6dav32Pz5833WJMmJjIx0jh496j124MABR5LzySefeI+NGjXKiYmJcU6dOuU9Vlxc7DRt2tTnnLfn2b17t89z+rfNmzc7kpw9e/b4PDZu3DinXbt2/1kfavS49h7fqbG8fU2fa3f9+nXnscceczp16uTcvHnTuL4+0eO69/j06dOOJO9/HTp0cDZu3Fin2rqq149ERUVFaerUqT7H77//fu//X7p0SefOnVP//v1VUVGhI0eO1HreCRMmKD4+3vt1//79JUnHjh2rtdbtdispKcn7dUpKiuLi4ry1VVVV2rlzp9LT09W+fXvvuC5dumj48OE+58vOzpbjOLX+5HT16lVJqvGPIKKjo72PNzT0ODzQ5/82c+ZM/f7771q2bJmaNq3XNyQDhh77atmypQoKCrR161bl5uaqdevWAf+0TL1+tzz00EOKjIz0OX7o0CHNnTtXu3bt0sWLF6s9Vl5eXut5H3nkkWpf3254aWmpce3t+tu1f//9t65evaouXbr4jKvpWF3d/sa+fv26z2PXrl2r9o3fkNDj8ECf727JkiVatWqVFixYoGeeeSZg561v9NhXZGSk3G63JGnkyJEaMmSI+vXrp7Zt22rkyJH3fH6pnkO5pqApKyvTwIEDFRcXp9zcXCUlJSk6Olr79u1TVlaWbt26Vet5IyIiajzu1OHTXvdSey8SEhIkSadPn/Z57PTp09V+ymtI6HF4oM81W716tbKysvTiiy9q7ty59TZvMNDj2vXt21cJCQlav359wwzlmhQWFur8+fPasmWLBgwY4D1eUlISwlX9X9u2bRUdHa2jR4/6PFbTsbpKTk5W06ZNtXfvXo0fP957/MaNG/J4PNWONXTh2uNwE+59zs/P1wsvvKAxY8Zo+fLl93w+G4V7j2ty7dq1Or1DUFchv83m7Z98/v2Tzo0bN/Tpp5+GaknVREREyO12Ky8vT3/99Zf3+NGjR2u8wUdd/8S+efPmcrvdWrdunS5duuQ9vnbtWl2+fFnjxo0L3JMIsXDtcbgJ5z7v2bNHEydO1IABA7R+/Xo1aRLyl9agCNceX7lypcYxX3/9tUpLS30+RXMvQn6l3LdvX8XHx2vKlCmaNWuWXC6X1q5da9Vbi9nZ2dqxY4f69eunl156SVVVVVq2bJmSk5Pl8XiqjTX5E/t3331Xffv21cCBA7139Prggw80dOhQDRs2LHhPqJ6Fc49//fVXffvtt5L+eWEoLy/XO++8I0nq0aOHRo0aFYynExLh2ucTJ07o2Weflcvl0vPPP6/NmzdXezwlJUUpKSlBeDb1L1x7XFxcLLfbrQkTJqhbt25q0qSJ9u7dq3Xr1ikxMVGvvPJKwNYf8lBu1aqVvvvuO73++uuaO3eu4uPjNWnSJA0ZMkRPP/10qJcnSerVq5e2bdumN954Q/PmzdPDDz+s3NxcHT58uE5/bXg3PXv21M6dO5WVlaVXX31VsbGxmjZtmt57770Arj70wrnH+/bt07x586odu/31lClTGlUoh2ufS0pKvG9fzpgxw+fx+fPnN5pQDtced+jQQWPHjtWuXbu0Zs0a3bx5Ux07dtTMmTP11ltveT8jHQgBufd1uEpPT9ehQ4dUXFwc6qUgSOhxeKDPjV9D6XHj/MVHENz5ueHi4mJ9//33Yf1Z1caGHocH+tz4NeQec6VcRwkJCcrIyFDnzp114sQJrVixQtevX9f+/fvVtWvXUC8PAUCPwwN9bvwaco9D/jvlhmLYsGHasGGDzpw5o6ioKPXp00cLFy60vsGoO3ocHuhz49eQe8yVMgAAluB3ygAAWIJQBgDAEoQyAACWsOoPve68E05dZGVlGdekpaUZ1yxatMho/L+3JsO98edjDGVlZcY1OTk5RuNHjx5tPAfurrCw0LgmPT3duCY1NdVovD/rCgeLFy82rpkzZ45xTadOnYxrioqKjMbb9HrNlTIAAJYglAEAsAShDACAJQhlAAAsQSgDAGAJQhkAAEsQygAAWIJQBgDAEoQyAACWIJQBALAEoQwAgCWsuve1P/exLikpMa4pLS01rmnZsqXR+E2bNhnPMW7cOOOacNCiRQvjmh9//NG4Zvfu3Ubjuff13Xk8HuOawYMHG9c0b97cuOb48ePGNeHA9L7U/rzGrVy50rgmMzPTuMb03tdut9t4jmDhShkAAEsQygAAWIJQBgDAEoQyAACWIJQBALAEoQwAgCUIZQAALEEoAwBgCUIZAABLEMoAAFiCUAYAwBJBvfe16f1H/bmP9R9//GFc07lzZ+OatLQ0o/Gmz10Kj3tf+3NP5MLCwoCvoyapqan1Mk84yMvLM67p0aOHcU16erpxTU5OjnFNOJg+fbrReH/2KujVq5dxTadOnYxrbLqXtSmulAEAsAShDACAJQhlAAAsQSgDAGAJQhkAAEsQygAAWIJQBgDAEoQyAACWIJQBALAEoQwAgCUIZQAALEEoAwBgiaBuSFFaWmo0vmfPnsZz+LO5hD/8uZF6OPjwww+NxmdnZxvPUV5eblzjj0GDBtXLPOFg9uzZxjWJiYn1Ms/o0aONa8KB6WvpsWPHjOfwZ9MhfzaXMM2e+Ph44zmChStlAAAsQSgDAGAJQhkAAEsQygAAWIJQBgDAEoQyAACWIJQBALAEoQwAgCUIZQAALEEoAwBgCUIZAABLEMoAAFjCqg0p0tLSgrSSe9eQb3AeTKYbAmRkZBjPUV//lmVlZfUyT0Nk+m9julGJJOXl5RnX+GP16tX1Mk9j589mQBcuXDCu8WdDCtOanTt3Gs8RrNclrpQBALAEoQwAgCUIZQAALEEoAwBgCUIZAABLEMoAAFiCUAYAwBKEMgAAliCUAQCwBKEMAIAlCGUAACxBKAMAYImgbkhhesPuoqKiIK2kOtPNJSRp7969RuPHjx9vPAdCy+PxGI1PTU0NyjpslJ2dbTT+o48+Cs5C7uDPJhYtWrQI+DpQN/5s4uDPZhGZmZlG4xcvXmw8x6JFi4xr6oIrZQAALEEoAwBgCUIZAABLEMoAAFiCUAYAwBKEMgAAliCUAQCwBKEMAIAlCGUAACxBKAMAYAlCGQAASwT13tedO3c2Gm96f2lJ2rx5c73UmMrKygr6HEB9ycjIMBpfWFhoPMeBAweMa9LT041rRo8ebTR+6tSpQZ+jIZozZ45xjdvtNq7xZ6+CgoICo/E27VXAlTIAAJYglAEAsAShDACAJQhlAAAsQSgDAGAJQhkAAEsQygAAWIJQBgDAEoQyAACWIJQBALAEoQwAgCUIZQAALGHVhhSLFy82nsOfjR969+5tXFNUVGRcA18tWrQwrvHn5v75+fnGNaabKJhu0tCQpaamGo33eDzGc/hTk52dbVxj+r2RmJhoPEc4bEgRHx9vXDN9+vQgrMSX6QYTK1euDNJKzHGlDACAJQhlAAAsQSgDAGAJQhkAAEsQygAAWIJQBgDAEoQyAACWIJQBALAEoQwAgCUIZQAALEEoAwBgCUIZAABLuBzHcUK9CAAAwJUyAADWIJQBALAEoQwAgCUIZQAALEEoAwBgCUIZAABLEMoAAFiCUAYAwBKEMgAAlvgfbeAcukpANTsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(6, 2))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans tous les exercices de reconnaissances, il est d'usage de diviser les données en deux ensembles: une partie pour l'apprentissage et l'autre pour vérifier le résultat. Voici la méthode que nous utiliserons pour récupérer ces deux ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_data():\n",
    "    digits = datasets.load_digits()\n",
    "    \n",
    "    # il faut mélanger les entrées, ce qu'on fait en mélangeant les indices\n",
    "    rng = np.random.RandomState(0)\n",
    "    indices = np.arange(len(digits.data))\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    # divide the dataset into learning et test sets\n",
    "    X = digits.data[indices[:300]]\n",
    "    y = digits.target[indices[:300]]\n",
    "    image = digits.images[:300]\n",
    "    \n",
    "    return X, y, image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre réseau de neurones et un jeu de données, faisons notre premier essai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance initiale : 6.67%\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, image_test = load_data()\n",
    "\n",
    "net = Network(input_dim=64)\n",
    "net.add_layer(200)\n",
    "# net.add_layer(10)\n",
    "\n",
    "accuracy = net.evaluate(X_test, y_test)\n",
    "print('Performance initiale : {:.2f}%'.format(accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous n'aurez sûrement pas une bonne performance avec ce premier essai: nous avons pris des poids au hasard et nous nous avons fait fonctionner le réseau sans lui avoir fait faire auparavant une phase d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction d'un réseau avec possibilité d'apprentissage.\n",
    "\n",
    "### Mise en place du réseau et des notations.\n",
    "\n",
    "Le [cahier suivant](https://www.miximum.fr/blog/introduction-au-deep-learning-2/) de Miximum expose comment introduire une notion d'apprentissage dans le réseau neuronal que nous avons construit. Pour réaliser un apprentissage, il faut une fonction de coût qui va nous dire de combien on est éloigné de la solution. Le cahier sur les [concepts](./00_concepts.ipynb) a exposé une méthode classique est bien connue qui est l'algorithme du gradient. Nous allons utiliser la même chose, mais en l'appliquant à ce réseau.\n",
    "\n",
    "Nous allons d'abord commencer par un rappel sur les notations.\n",
    "\n",
    "![réseau neuronal](./img/reseau_poids.png)\n",
    "\n",
    "\n",
    "- $L$ est le nombre de couches du neurones.\n",
    "- $\\sigma$ est la fonction d'activation Sigmoïde.\n",
    "- $w_{ij}$ est le poids qui relie le $i^\\text{ième}$ neurone de la $l^\\text{ième}$ couche au $j^\\text{ième}$ neurone de la $(l - 1)^\\text{ième}$ couche.\n",
    "- $w^l$ est une matrice de taille $i\\times j$ ($i$ lignes et $j$ colonnes) qui contient tous les poids de la $l^\\text{ième}$ couche.\n",
    "- $b_i^l$ est le biais associé au $i^\\text{ième}$ neurone de la $l^\\text{ième}$ couche.\n",
    "- $b^l$ est le vecteur qui contient tous les biais de la $l^\\text{ième}$ couche.\n",
    "\n",
    "$Z_i^l$ est la valeur d'aggrégation du $i^\\text{ième}$ neurone de la $l^\\text{ième}$ couche, c'est-à-dire la valeur qu'un neurone calcule avant de passer à la fonction d'activation.\n",
    "\n",
    "$$ z_i^l = \\sum_{j=1}^m w_{ij}^l a_j^{l-1} + b_i^l$$\n",
    "\n",
    "$a_i^l$ est la valeur d'activation du $i^\\text{ième}$ neurone de la $l^\\text{ième}$ couche, c'est-à-dire la valeur définitive retournée par le neurone : $ a_i^l = \\sigma(z_l^l)$.\n",
    "\n",
    "En utilisant la notation vectorielle (les vecteurs sont notés par des majuscules),\n",
    "\n",
    "$$ \n",
    "Z^l = \\left(\\begin{array}{c}z_1^l \\\\ z_2^l \\\\ \\vdots \\\\ z_n^l \\end{array}\\right)\n",
    "\\quad \\quad\n",
    "A^l = \\left(\\begin{array}{c}a_1^l \\\\ a_2^l \\\\ \\vdots \\\\ a_n^l \\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "on peut écrire.\n",
    "\n",
    "$$\\begin{align}\n",
    "Z^l &= W^l \\circ A^{l-1} + B^l \\\\\n",
    "A^l &= \\sigma(Z^l) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Pour chaque couche on passe la première équation qui est l'agrégation sur tous les neurones de la couche (ce qu'on peut faire une seule opération en utilisant une notation vectorielle) et en appliquant ensuite le filtre d'activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluer une estimation.\n",
    "\n",
    "Le réseau peut être représenté ainsi: à partir d'une entrée $X$ on obtient une sortie $Y$ et l'objectif est que cette sortie soit la plus proche possible de $T$, la cible (la *target*).\n",
    "\n",
    "![réseau en forward](./img/reseau_XYT.png)\n",
    "\n",
    "Si le réseau n'est pas entraîné (comme dans notre exemple précédent avec des poids de départ aléatoires), il y a beaucoup de chance que $Y$ soit différent de $T$. On va caractériser la distance à notre cible avec une fonction de la forme suivante.\n",
    "\n",
    "$$ C(W) = \\frac{1}{2m} (Y - T)^2$$\n",
    "\n",
    "(revoir le cahier sur les [concepts](00_concepts.ipynb))\n",
    "\n",
    "Nous savons bien évaluer cette erreur entre le résultat sortant de la dernière couche du réseau et la cible, mais il va falloir ensuite propager cette évaluation d'erreur en arrière dans les couches du réseau.\n",
    "\n",
    "![réseau rétropropagation](./img/reseau_backward.png)\n",
    "\n",
    "Chaque couche reçoit donc une erreur $\\delta^{l}$ et on applique alors l'algorithme du gradient pour changer les coefficients à l'intérieur de cette couche. C'est le calcul de rétro-propagation du gradient.\n",
    "\n",
    "On voit aussi le nombre de calcul nécessaire et il faut donc pouvoir faire des calculs efficaces pour espérer avoir un résultat en un temps raisonnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Répartir l'erreur sur la dernière couche.\n",
    "\n",
    "Dès qu'on change un poids, tout le réseau est impacté.\n",
    "\n",
    "![réseau et impact d'un changement](./img/reseau_impact.png)\n",
    "\n",
    "Imaginons une modification très petite sur le poids $w_{11}^L$ que l'on note $dw_{11}^L$, cette modification aura un impact sur le résultat :\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w_{11}^L}dw_{11}^l = dC $$\n",
    "\n",
    "Mais cette modification va aussi impacter le résultat élémentaire $z_1^L de la façon suivante.\n",
    "\n",
    "$$ \\frac{\\partial z_1^L}{\\partial w_{11}^L} dw_{11}^l = dz_1^L$$\n",
    "\n",
    "et l'impact sur $z_1^L$ aura un impact sur $a_1^L$\n",
    "\n",
    "$$ \\frac{\\partial a_1^L}{\\partial z_1^L} dz_1^l = da_1^L$$\n",
    "\n",
    "et puisque la la sortie dépend de $a_1^L$, la fonction de coût aussi.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial a_1^L} da_1^L = dC $$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial a_1^L} = \\frac{}{m}(Y - T) $$\n",
    "\n",
    "On peut combiner les équations précédentes pour obtenir.\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w_{11}^L} =\\frac{\\partial z_1^L}{\\partial w_{11}^L} \n",
    "                                         \\frac{\\partial a_1^L}{\\partial z_1^L}\n",
    "                                         \\frac{\\partial C}{\\partial a_1^L}$$\n",
    "\n",
    "La dérivée $\\displaystyle \\partial C / \\partial a_1^L$ est la dérivée de la fonction de coût qui sera simple à calculer. Dans le [cahier sur les concepts](00_concepts.ipynb) nous avons vu que pour cette dernière couche on pourra calculer\n",
    "\n",
    "$$\\frac{dC}{dW} = \\frac{1}{m} (Y - T)^T \\circ (Y - T)$$\n",
    "\n",
    "avec la définition que nous avons donnée.\n",
    "\n",
    "La dérivée $\\partial a_1^L / \\partial z_1^L$ est la dérivée de la fonction d'activation. Si on utilise la fonction Sigmoïde $\\sigma$, on sait que le résultat sera simple.\n",
    "\n",
    "$$\\frac{\\partial a_1^L}{\\partial z_1^L} = \\sigma'(z_1^L) = \\sigma(z_1^L) \\big(1 - \\sigma(z_1^L)\\big)$$\n",
    "\n",
    "La dérivée $\\partial z_1^L / \\partial w_{11}^L$ est la variation de la fonction d'agrégation en fonction de ce seul poids, donc le résultat est aussi simple.\n",
    "\n",
    "$$\\frac{\\partial z_1^L}{\\partial w_{11}^L} = a_1^{L-1}$$\n",
    "\n",
    "Et donc en récapitulant, on obtient l'équation suivante.\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w_{11}^L} = a_1^{L-1} \n",
    "                                          \\sigma'(z_1^L)                                          \\frac{dC}{da_1^L}$$\n",
    "\n",
    "Or la variation $\\partial C / \\partial z_1^L$ est la composante $1$ de l'erreur $\\delta^L$.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_1^L} = \n",
    "     \\frac{\\partial a_1^L}{\\partial z_1^L}\\frac{\\partial C}{\\partial a_1^L} =\n",
    "     \\sigma'(z_1^L) \\frac{dC}{da_1^L} = \\delta_1^L$$\n",
    "\n",
    "On obtient finalement\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{ij}^L} = a_j^{L-1} \\cdot \\delta_i^L$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rétro-propagager le calcul des erreurs.\n",
    "\n",
    "Nous allons maintenant voir comment retro-propager ce calcul d'erreur sur les autres couches.\n",
    "\n",
    "![réseau et impact d'un changement](./img/reseau_impact.png)\n",
    "\n",
    "Voici par exemple sur la quatrième couche.\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w_{12}^4} = \\frac{\\partial z_1^4}{\\partial w_{12}^4}\n",
    "                                          \\frac{\\partial a_1^4}{\\partial z_1^4}\n",
    "                                          \\frac{C}{\\partial a_1^4}$$\n",
    "\n",
    "Le premier terme $\\partial z_1^4 / \\partial w_{12}^4$ est la variation de la fonction d'agrégation, donc on a \n",
    "\n",
    "$$\\frac{\\partial z_1^4}{\\partial w_{12}^4} = a_2^3$$\n",
    "\n",
    "Le second terme $\\partial a_1^4 / \\partial z_1^4$ désigne la variation de la fonction d'activation.\n",
    "\n",
    "$$\\frac{\\partial a_1^4}{\\partial z_1^4} = \\sigma'(z_1^4)$$\n",
    "\n",
    "Pour le dernier terme $\\partial C / \\partial a_1^4$ on remarque que s'il est modifié, il aura un impact sur $z_1^L$, $z_2^L$ et $z_3^L$ et on va donc écrire.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial a_1^4} = \\frac{\\partial z_1^L}{\\partial a_1^4}\n",
    "                                      \\frac{\\partial C}{\\partial z_1^L} +\n",
    "                                      \\frac{\\partial z_2^L}{\\partial a_1^4}\n",
    "                                      \\frac{\\partial C}{\\partial z_2^L} +\n",
    "                                      \\frac{\\partial z_3^L}{\\partial a_1^4}\n",
    "                                      \\frac{\\partial C}{\\partial z_3^L}\n",
    "                                    = \\sum_k \\frac{\\partial z_k^L}{\\partial a_1^4}\n",
    "                                             \\frac{\\partial C}{\\partial z_k^L}$$\n",
    "\n",
    "En sachant qu'on a vu que \n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial z_k^L} = \\delta_k^L$$\n",
    "\n",
    "et pour l'autre terme, on vérifie que\n",
    "\n",
    "$$\\frac{\\partial z_k^L}{\\partial a_1^4} = w_{k1}^L$$\n",
    "\n",
    "et donc \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_{12}^4} = a_2^3 \\cdot \\sigma'(z_1^4) \\sum_k w_{k1}^L \\delta_k^L$$\n",
    "\n",
    "On peut ensuite généraliser à tous les termes. Il existe un [article en anglais](http://neuralnetworksanddeeplearning.com/chap2.html#the_four_fundamental_equations_behind_backpropagation) donnant une explication détaillée du calcul de rétro-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La deuxième version avec apprentissage.\n",
    "\n",
    "Pour la deuxième version avec apprentissage, nous revoyons la fonction de récupération des données, qui seront séparées en un échantillon pour l'apprentissage et une version pour tester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    digits = datasets.load_digits()\n",
    "    \n",
    "    # shuffle the datasets\n",
    "    rng = np.random.RandomState(0)\n",
    "    indices = np.arange(len(digits.data))\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    # divide the dataset into learning et test sets\n",
    "    X_learn = digits.data[indices[:330]]\n",
    "    y_learn = digits.target[indices[:330]]\n",
    "    X_test = digits.data[indices[331:]]\n",
    "    y_test = digits.target[indices[331:]]\n",
    "\n",
    "    return X_learn, y_learn, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu que nous aurons besoin de la dérivée de la fonction sigmoïde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    \"\"\"Dérivée de la fonction sigmoid.\"\"\"\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On redéfinit une classe `layer` avec la possibilité de retro-propagation, qui est donnée par les trois dernières méthodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer2:\n",
    "    \"\"\"Une seule couche de neurones.\"\"\"\n",
    "    def __init__(self, size, input_size):\n",
    "        self.size = size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Les poids sont représentés par une matrice de n lignes\n",
    "        # et m colonnes. n = le nombre de neurones, m = le nombre de\n",
    "        # neurones dans la couche précédente.\n",
    "        self.weights = np.random.randn(size, input_size)\n",
    "\n",
    "        # Un biais par neurone\n",
    "        self.biases = np.random.randn(size)\n",
    "\n",
    "    # Résultat du calcul de chaque neurone.\n",
    "    # Il est important de noter que `data` est un vecteur (normalement, de\n",
    "    # longueur `self.input_size`, et que nous retournons un vecteur de\n",
    "    # taille `self.size`.\n",
    "    def forward(self, data):\n",
    "        aggregation = self.aggregation(data)\n",
    "        activation = self.activation(aggregation)\n",
    "        return activation\n",
    "\n",
    "    # Calcule la somme des entrées pondérées + biais pour chaque neurone.\n",
    "    # Plutôt que d'utiliser une boucle for, nous tirons parti du calcul\n",
    "    # matriciel qui permet d'effectuer toutes ces opérations d'un coup.\n",
    "    def aggregation(self, data):\n",
    "        return np.dot(self.weights, data) + self.biases\n",
    "\n",
    "    # Passe les valeurs aggrégées dans la moulinette de la fonction\n",
    "    # d'activation.\n",
    "    # `x` est un vecteur de longueur `self.size`, et nous retournons un\n",
    "    # vecteur de même dimension.\n",
    "    def activation(self, x):\n",
    "        return sigmoid(x)\n",
    "\n",
    "    # Dérivée de la fonction d'activation.\n",
    "    def activation_prime(self, x):\n",
    "        return sigmoid_prime(x)\n",
    "\n",
    "    # Mise à jour des poids à partir du gradient (algo du gradient)\n",
    "    def update_weights(self, gradient, learning_rate):\n",
    "        self.weights -= learning_rate * gradient\n",
    "\n",
    "    # Idem mais avec les biais\n",
    "    def update_biases(self, gradient, learning_rate):\n",
    "        self.biases -= learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La programmation du réseau est la partie la plus modifiée. Nous aurons besoin pour cette méthode d'une fonction `one_hot` qui ne conserve qu'une des coordonnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, k):\n",
    "    \"\"\"Convertit un entier en vecteur \"one-hot\".\n",
    "\n",
    "    to_one_hot(5, 10) -> (0, 0, 0, 0, 1, 0, 0, 0, 0)\n",
    "\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros(k)\n",
    "    one_hot[y] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2:\n",
    "    \"\"\"Un réseau constitué de couches de neurones.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, size):\n",
    "        if len(self.layers) > 0:\n",
    "            input_dim = self.layers[-1].size\n",
    "        else:\n",
    "            input_dim = self.input_dim\n",
    "        self.layers.append(Layer2(size, input_dim))\n",
    "\n",
    "    # Propage les données d'entrée d'une couche à l'autre.\n",
    "    def feedforward(self, input_data):\n",
    "        activation = input_data\n",
    "        for layer in self.layers:\n",
    "            activation = layer.forward(activation)\n",
    "        return activation\n",
    "\n",
    "    # Retourne l'index du neurone de sortie qui a la plus haute valeur, ce\n",
    "    # qui revient à indiquer quelle classe est sélectionnée par le réseau.\n",
    "    def predict(self, input_data):\n",
    "        return np.argmax(self.feedforward(input_data))\n",
    "\n",
    "    # Évalue la performance du réseau à partir d'un set d'exemples.\n",
    "    # Retourne un nombre entre 0 et 1.\n",
    "    def evaluate(self, X, Y):\n",
    "        results = [1 if self.predict(x) == y else 0 for (x, y) in zip(X, Y)]\n",
    "        accuracy = sum(results) / len(results)\n",
    "        return accuracy\n",
    "\n",
    "    # Fonction d'entraînement du modèle qui va permettre de mettre à jour les poids.\n",
    "    def train(self, X, Y, steps=30, learning_rate=0.3, batch_size=10):\n",
    "        n = Y.size\n",
    "        for i in range(steps):\n",
    "            # Mélangeons les données parce que… parce que.\n",
    "            X, Y = shuffle(X, Y)\n",
    "            for batch_start in range(0, n, batch_size):\n",
    "                X_batch, Y_batch = X[batch_start:batch_start + batch_size], Y[batch_start:batch_start + batch_size]\n",
    "                self.train_batch(X_batch, Y_batch, learning_rate)\n",
    "\n",
    "    # Cette fonction combine les algos du rétro-propagation du gradient +\n",
    "    # gradient descendant.\n",
    "    def train_batch(self, X, Y, learning_rate):\n",
    "        # Initialise les gradients pour les poids et les biais.\n",
    "        weight_gradient = [np.zeros(layer.weights.shape) for layer in self.layers]\n",
    "        bias_gradient = [np.zeros(layer.biases.shape) for layer in self.layers]\n",
    "\n",
    "        # On fait tourner l'algo de rétro-propagation pour calculer les\n",
    "        # gradients un certain nombre de fois. On fera la moyenne ensuite.\n",
    "        for (x, y) in zip(X, Y):\n",
    "            new_weight_gradient, new_bias_gradient = self.backprop(x, y)\n",
    "            weight_gradient = [wg + nwg for wg, nwg in zip(weight_gradient, new_weight_gradient)]\n",
    "            bias_gradient = [bg + nbg for bg, nbg in zip(bias_gradient, new_bias_gradient)]\n",
    "\n",
    "        # C'est ici qu'on calcule les moyennes des gradients calculés\n",
    "        avg_weight_gradient = [wg / Y.size for wg in weight_gradient]\n",
    "        avg_bias_gradient = [bg / Y.size for bg in bias_gradient]\n",
    "\n",
    "        # Il ne reste plus qu'à mettre à jour les poids et biais en\n",
    "        # utilisant l'algo du gradient descendant.\n",
    "        for layer, weight_gradient, bias_gradient in zip(self.layers,\n",
    "                                                         avg_weight_gradient,\n",
    "                                                         avg_bias_gradient):\n",
    "            layer.update_weights(weight_gradient, learning_rate)\n",
    "            layer.update_biases(bias_gradient, learning_rate)\n",
    "\n",
    "    # L'algorithme de rétro-propagation du gradient.\n",
    "    # C'est là que tout le boulot se fait.\n",
    "    def backprop(self, x, y):\n",
    "\n",
    "        # On va effectuer une passe vers l'avant, une passe vers l'arrière\n",
    "        # On profite de la passe vers l'avant pour stocker les calculs\n",
    "        # intermédiaires, qui seront réutilisés par la suite.\n",
    "        aggregations = []\n",
    "        activation = x\n",
    "        activations = [activation]\n",
    "\n",
    "        # Propagation pour obtenir la sortie\n",
    "        for layer in self.layers:\n",
    "            aggregation = layer.aggregation(activation)\n",
    "            aggregations.append(aggregation)\n",
    "            activation = layer.activation(aggregation)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Calculons la valeur delta (δ) pour la dernière couche\n",
    "        # en appliquant les équations détaillées plus haut.\n",
    "        target = to_one_hot(int(y), 10)\n",
    "        delta = self.get_output_delta(aggregation, activation, target)\n",
    "        deltas = [delta]\n",
    "\n",
    "        # Phase de rétropropagation pour calculer les deltas de chaque\n",
    "        # couche\n",
    "        # On utilise une implémentation vectorielle des équations.\n",
    "        nb_layers = len(self.layers)\n",
    "        for l in reversed(range(nb_layers - 1)):\n",
    "            layer = self.layers[l]\n",
    "            next_layer = self.layers[l + 1]\n",
    "            activation_prime = layer.activation_prime(aggregations[l])\n",
    "            delta = activation_prime * np.dot(next_layer.weights.transpose(), delta)\n",
    "            deltas.append(delta)\n",
    "\n",
    "        # Nous sommes parti de l'avant-dernière couche pour remonter vers\n",
    "        # la première. deltas[0] contient le delta de la dernière couche.\n",
    "        # Nous l'inversons pour faciliter la gestion des indices plus tard.\n",
    "        deltas = list(reversed(deltas))\n",
    "\n",
    "        # On utilise maintenant les deltas pour calculer les gradients.\n",
    "        weight_gradient = []\n",
    "        bias_gradient = []\n",
    "        for l in range(len(self.layers)):\n",
    "\n",
    "            # Notez que l'indice des activations est « décalé », puisque\n",
    "            # activation[0] contient l'entrée (x), et pas l'activation de\n",
    "            # la première couche.\n",
    "            prev_activation = activations[l]\n",
    "            weight_gradient.append(np.outer(deltas[l], prev_activation))\n",
    "            bias_gradient.append(deltas[l])\n",
    "\n",
    "        return weight_gradient, bias_gradient\n",
    "\n",
    "    # Calcule le delta pour la dernière couche, en utilisant\n",
    "    # les dernières valeurs d'aggregation, d'activation, et la valeur\n",
    "    # cible.\n",
    "    # Notez que lorsque l'on utilise l'entropie croisée pour fonction de\n",
    "    # coût, l'équation de calcul de delta peut-être simplifiée pour aboutir\n",
    "    # au résultat ci dessous.\n",
    "    # Cf http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function\n",
    "    def get_output_delta(self, z, a, target):\n",
    "        return a - target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons maintenant de nouveau le réseau en faisant cette fois une partie apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance initiale : 16.92%\n",
      "Nouvelle performance : 68.14%\n",
      "Nouvelle performance : 82.13%\n",
      "Nouvelle performance : 74.69%\n",
      "Nouvelle performance : 76.47%\n",
      "Nouvelle performance : 87.24%\n",
      "Nouvelle performance : 88.88%\n",
      "Nouvelle performance : 90.31%\n",
      "Nouvelle performance : 82.74%\n",
      "Nouvelle performance : 86.56%\n",
      "Nouvelle performance : 87.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52030/4233617500.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "X_learn, y_learn, X_test, y_test = load_data()\n",
    "\n",
    "net = Network2(input_dim=64)\n",
    "# net.add_layer(200)\n",
    "net.add_layer(10)\n",
    "\n",
    "accuracy = net.evaluate(X_test, y_test)\n",
    "print('Performance initiale : {:.2f}%'.format(accuracy * 100.0))\n",
    "\n",
    "for i in range(10):\n",
    "    net.train(X_learn, y_learn, steps=1, learning_rate=3.0)\n",
    "    accuracy = net.evaluate(X_test, y_test)\n",
    "    print('Nouvelle performance : {:.2f}%'.format(accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
