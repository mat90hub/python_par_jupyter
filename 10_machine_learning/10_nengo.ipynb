{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance de caractères avec [Nengo](https://www.nengo.ai/nengo-dl/index.html)\n",
    "\n",
    "\n",
    "Cette page est tiré d'un [tutoriel de Nengo](https://www.nengo.ai/nengo-dl/examples/spiking-mnist.html) qui montre comment reconnaître des caractères écrits avec l'extension [Nengo](https://www.nengo.ai/nengo-dl/index.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce tutoriel utilise aussi l'extension [Tensorflow](https://www.tensorflow.org/?hl=fr), développée par [Google Brain Team](https://research.google.com/teams/brain/?hl=he) pour le *deep learnin* et le *machine learning*. On trouve des tutoriels [ici](https://pythonguides.com/tensorflow/). \n",
    "\n",
    "Dans ce module, on utilise les [fichiers de données](https://keras.io/api/datasets/) d'entraînement du sous-module [Kera](https://keras.io/about/), plus exactement des images de  [chiffres manuscrits](https://keras.io/api/datasets/mnist/) traduites sous formes de matrices de points utilisables directement sous Python. Ceci nous économise la phase d'acquisition et de traitement d'image, qu'il faudrait faire sinon avec par exemple l'extension [Scikit-image](https://scikit-image.org/docs/stable/user_guide/numpy_images.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nengo\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo_dl\n",
    "\n",
    "(train_images, train_labels), (\n",
    "    test_images,\n",
    "    test_labels,\n",
    ") = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'extension [matplotlib.pyplot](https://matplotlib.org/stable/tutorials/introductory/quick_start.html) propose la commande [imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) qui permet d'afficher des matrices de points, représentant donc ici l'image d'un chiffre manuscrit. Ci-dessous, l'une des images de la liste de matrices `train_images` est affichée en précisant son étiquette (elles sont stockées dans la liste `train_labels`) et sa taille en nombre de points avec la méthode [shape](https://www.digitalocean.com/community/tutorials/python-shape-method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La taille de l'image est : (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = 5    # changer ce chiffre pour une autre image\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(train_images[num], cmap =\"gray\")\n",
    "plt.title(str(train_labels[num]))\n",
    "plt.axis(\"off\")\n",
    "print(f'La taille de l\\'image est : {test_images[num].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour affichier plus d'images, il suffit de faire une boucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathieu/Sync/informatique/programmation/python/python_par_jupyter/.venv/lib/python3.10/site-packages/nengo_dl/simulator.py:2066: RuntimeWarning: Simulator with model=Model: <Network (unlabeled) at 0x7fb4a930cf10>, dt=0.001000 was deallocated while open. Simulators should be closed manually to ensure resources are properly freed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# affichons les premiers chiffres\n",
    "plt.figure(figsize=(12,2))\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    plt.subplot(1,n,i+1)\n",
    "    plt.imshow(train_images[i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(str(train_labels[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la suite de l'exercice, ces images sont [applaties](https://www.w3schools.com/python/numpy/numpy_array_reshape.asp). Elles pourront toujours être réaffichées en changeant la forme avant l'affichage avec [numpy.reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# on applatit les images\n",
    "train_images = train_images.reshape((train_images.shape[0],-1))\n",
    "test_images = test_images.reshape((test_images.shape[0],-1))\n",
    "\n",
    "# malgré tout, on peut toujours afficher ces images\n",
    "plt.figure(figsize=(12,2))\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    plt.subplot(1,n,i+1)\n",
    "    # pour afficher les images applaties, on les \"re-forme\"\n",
    "    plt.imshow(np.reshape(train_images[i],(28,28)), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(str(train_labels[i]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la phase suivante, on définit le réseau neuronal à utiliser avec Nengo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nengo.Network(seed=0) as net:\n",
    "    # set some default parameters for the neurons that will make\n",
    "    # the training progress more smoothly\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([100])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    neuron_type = nengo.LIF(amplitude=0.01)\n",
    "\n",
    "    # this is an optimization to improve the training speed,\n",
    "    # since we won't require stateful behaviour in this example\n",
    "    nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    # the input node that will be used to feed in input images\n",
    "    inp = nengo.Node(np.zeros(28 * 28))\n",
    "\n",
    "    # add the first convolutional layer\n",
    "    x = nengo_dl.Layer(tf.keras.layers.Conv2D(filters=32, kernel_size=3))(\n",
    "        inp, shape_in=(28, 28, 1)\n",
    "    )\n",
    "    x = nengo_dl.Layer(neuron_type)(x)\n",
    "\n",
    "    # add the second convolutional layer\n",
    "    x = nengo_dl.Layer(tf.keras.layers.Conv2D(filters=64, strides=2, kernel_size=3))(\n",
    "        x, shape_in=(26, 26, 32)\n",
    "    )\n",
    "    x = nengo_dl.Layer(neuron_type)(x)\n",
    "\n",
    "    # add the third convolutional layer\n",
    "    x = nengo_dl.Layer(tf.keras.layers.Conv2D(filters=128, strides=2, kernel_size=3))(\n",
    "        x, shape_in=(12, 12, 64)\n",
    "    )\n",
    "    x = nengo_dl.Layer(neuron_type)(x)\n",
    "\n",
    "    # linear readout\n",
    "    out = nengo_dl.Layer(tf.keras.layers.Dense(units=10))(x)\n",
    "\n",
    "    # we'll create two different output probes, one with a filter\n",
    "    # (for when we're simulating the network over time and\n",
    "    # accumulating spikes), and one without (for when we're\n",
    "    # training the network using a rate-based approximation)\n",
    "    out_p = nengo.Probe(out, label=\"out_p\")\n",
    "    out_p_filt = nengo.Probe(out, synapse=0.1, label=\"out_p_filt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous construisons maintenant un premier simulateur pour ce réseau en utilisant [NegoDl](https://www.nengo.ai/nengo-dl/user-guide.html) qui utilise [Nengo](https://www.nengo.ai/nengo/user-guide.html) et [Keras](https://www.tensorflow.org/guide/keras/train_and_evaluate?hl=fr), lui même venant de l'extension [TensoFlow](https://www.tensorflow.org/?hl=fr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                     Building network (0%)                    | ETA:  --:--:--\n",
      "|###                   Building network (5%)                     | ETA: 0:00:01\n",
      "Build finished in 0:00:00\n",
      "|#                         Optimizing graph                           | 0:00:00\n",
      "|#             Optimizing graph: operator simplificaton               | 0:00:00\n",
      "Optimizing graph: operator simplificaton finished in 0:00:00\n",
      "|#                Optimizing graph: merging operators                 | 0:00:00\n",
      "Optimizing graph: merging operators finished in 0:00:00\n",
      "|#                Optimizing graph: ordering signals                  | 0:00:00\n",
      "Optimizing graph: ordering signals finished in 0:00:00\n",
      "|#                Optimizing graph: creating signals                  | 0:00:00\n",
      "Optimizing graph: creating signals finished in 0:00:00\n",
      "Optimization finished in 0:00:00\n",
      "|#                        Constructing graph                          | 0:00:00\n",
      "| #                       Constructing graph                          | 0:00:00\n",
      "|           Constructing graph: pre-build stage (0%)           | ETA:  --:--:--\n",
      "Constructing graph: pre-build stage finished in 0:00:00\n",
      "|             Constructing graph: build stage (0%)             | ETA:  --:--:--\n",
      "|##############Constructing graph: build stage (26%)             | ETA: 0:00:00\n",
      "|##############Constructing graph: build stage (89%)######       | ETA: 0:00:00\n",
      "|  #                      Constructing graph                          | 0:00:00\n",
      "Constructing graph: build stage finished in 0:00:00\n",
      "Construction finished in 0:00:00\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 200\n",
    "sim = nengo_dl.Simulator(net, minibatch_size=minibatch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous entrons nos données de test et d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add single timestep to training data\n",
    "train_images = train_images[:, None, :]\n",
    "train_labels = train_labels[:, None, None]\n",
    "\n",
    "# when testing our network with spiking neurons we will need to run it\n",
    "# over time, so we repeat the input/target data for a number of\n",
    "# timesteps.\n",
    "n_steps = 30\n",
    "test_images = np.tile(test_images[:, None, :], (1, n_steps, 1))\n",
    "test_labels = np.tile(test_labels[:, None, None], (1, n_steps, 1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour mesure la performance de notre reconnaissance, nous utilisons une fonction développée ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_accuracy(y_true, y_pred):\n",
    "    return tf.metrics.sparse_categorical_accuracy(y_true[:, -1], y_pred[:, -1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait un premier calcul de la qualité de la reconnaissance avant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|           Constructing graph: pre-build stage (0%)           | ETA:  --:--:--\n",
      "Constructing graph: pre-build stage finished in 0:00:00\n",
      "|             Constructing graph: build stage (0%)             | ETA:  --:--:--\n",
      "|############# Constructing graph: build stage (21%)             | ETA: 0:00:00\n",
      "|##############Constructing graph: build stage (63%)             | ETA: 0:00:00\n",
      "|############Constructing graph: build stage (100%)############| ETA:  00:00:00\n",
      "Constructing graph: build stage finished in 0:00:00\n",
      "|           Constructing graph: pre-build stage (0%)           | ETA:  --:--:--\n",
      "Constructing graph: pre-build stage finished in 0:00:00\n",
      "|             Constructing graph: build stage (0%)             | ETA:  --:--:--\n",
      "|##########    Constructing graph: build stage (15%)             | ETA: 0:00:00\n",
      "|##############Constructing graph: build stage (47%)             | ETA: 0:00:00\n",
      "|############Constructing graph: build stage (100%)############| ETA:  00:00:00\n",
      "Constructing graph: build stage finished in 0:00:00\n",
      "Accuracy before training: 0.11479999870061874\n"
     ]
    }
   ],
   "source": [
    "# note that we use `out_p_filt` when testing (to reduce the spike noise)\n",
    "sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "print(\n",
    "    \"Accuracy before training:\",\n",
    "    sim.evaluate(test_images, {out_p_filt: test_labels}, verbose=0)[\"loss\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous sommes prêt à entrainer notre réseau. Pour que l'exemple reste assez rapide, nous téléchargeons des poids qui auront été précalculés. Pour ce téléchargement, nous utilisaons l'extension [urllib](https://docs.python.org/3/library/urllib.html) qui regroupe des commandes pour manipuler les [url](https://developer.mozilla.org/fr/docs/Learn/Common_questions/Web_mechanics/What_is_a_URL). Dans cette extension le module [urllib.request](https://docs.python.org/3/library/urllib.request.html#module-urllib.request) définit une classe qui aide l'ouverture des URL.\n",
    "\n",
    "On peut aussi s'en passer et refaire les calculs depuis zéro en entrant `do_training=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "do_training = False\n",
    "if do_training:\n",
    "    # run training\n",
    "    sim.compile(\n",
    "        optimizer=tf.optimizers.RMSprop(0.001),\n",
    "        loss={out_p: tf.losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
    "    )\n",
    "    sim.fit(train_images, {out_p: train_labels}, epochs=10)\n",
    "\n",
    "    # save the parameters to file\n",
    "    sim.save_params(\"./mnist_params\")\n",
    "else:\n",
    "    # download pretrained weights\n",
    "    urlretrieve(\n",
    "        \"https://drive.google.com/uc?export=download&\"\n",
    "        \"id=1l5aivQljFoXzPP5JVccdFXbOYRv3BCJR\",\n",
    "        \"mnist_params.npz\",\n",
    "    )\n",
    "\n",
    "    # load parameters\n",
    "    sim.load_params(\"./mnist_params\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant nous pouvons vérifier la précision de notre classification avec ces paramètres qui ont été entrainés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "node data: should have rank 3 (batch_size, n_steps, dimensions), found rank 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m sim\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m{out_p_filt: classification_accuracy})\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m      3\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mAccuracy after training:\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m     sim\u001b[39m.\u001b[39;49mevaluate(test_images, {out_p_filt: test_labels}, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/Sync/informatique/programmation/python/python_par_jupyter/.venv/lib/python3.10/site-packages/nengo/utils/magic.py:179\u001b[0m, in \u001b[0;36mBoundFunctionWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrapper(wrapped, instance, args, kwargs)\n\u001b[1;32m    178\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__wrapped__, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstance, args, kwargs)\n\u001b[1;32m    180\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     instance \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__wrapped__, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Sync/informatique/programmation/python/python_par_jupyter/.venv/lib/python3.10/site-packages/nengo_dl/simulator.py:63\u001b[0m, in \u001b[0;36mrequire_open\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m instance\u001b[39m.\u001b[39mclosed:\n\u001b[1;32m     59\u001b[0m     \u001b[39mraise\u001b[39;00m SimulatorClosed(\n\u001b[1;32m     60\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot call \u001b[39m\u001b[39m{\u001b[39;00mwrapped\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m after simulator is closed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Sync/informatique/programmation/python/python_par_jupyter/.venv/lib/python3.10/site-packages/nengo_dl/simulator.py:895\u001b[0m, in \u001b[0;36mSimulator.evaluate\u001b[0;34m(self, x, y, n_steps, stateful, **kwargs)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[39m@require_open\u001b[39m\n\u001b[1;32m    863\u001b[0m \u001b[39m@fill_docs\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_steps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstateful\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    864\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(\u001b[39mself\u001b[39m, x\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, n_steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stateful\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    865\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[39m    Compute the loss and metric values for the network.\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39m        ``outputs[\"probe_name_loss\"]`` or ``outputs[\"probe_name_metric_name\"]``.\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_keras(\n\u001b[1;32m    896\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mevaluate\u001b[39;49m\u001b[39m\"\u001b[39;49m, x\u001b[39m=\u001b[39;49mx, y\u001b[39m=\u001b[39;49my, n_steps\u001b[39m=\u001b[39;49mn_steps, stateful\u001b[39m=\u001b[39;49mstateful, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    897\u001b[0m     )\n",
      "File \u001b[0;32m~/Sync/informatique/programmation/python/python_par_jupyter/.venv/lib/python3.10/site-packages/nengo/utils/magic.py:179\u001b[0m, in \u001b[0;36mBoundFunctionWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrapper(wrapped, instance, args, kwargs)\n\u001b[1;32m    178\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__wrapped__, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstance, args, kwargs)\n\u001b[1;32m    180\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     instance \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__wrapped__, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Sync/informatique/programmation/python/python_par_jupyter/.venv/lib/python3.10/site-packages/nengo_dl/simulator.py:46\u001b[0m, in \u001b[0;36mwith_self\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(instance\u001b[39m.\u001b[39mtensor_graph\u001b[39m.\u001b[39mdevice):\n\u001b[0;32m---> 46\u001b[0m         output \u001b[39m=\u001b[39m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     47\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mset_floatx(keras_dtype)\n",
      "File \u001b[0;32m~/Sync/informatique/programmation/python/python_par_jupyter/.venv/lib/python3.10/site-packages/nengo_dl/simulator.py:953\u001b[0m, in \u001b[0;36mSimulator._call_keras\u001b[0;34m(self, func_type, x, y, n_steps, stateful, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[39m# TODO: apply standardize/generate/check data to generator somehow\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[39m# maybe move it into a callback where the generated data is available?\u001b[39;00m\n\u001b[1;32m    952\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_inputs(x, n_steps\u001b[39m=\u001b[39mn_steps)\n\u001b[0;32m--> 953\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_data(\n\u001b[1;32m    954\u001b[0m     x,\n\u001b[1;32m    955\u001b[0m     n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[1;32m    956\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminibatch_size \u001b[39mif\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mon_batch\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m func_type \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    957\u001b[0m )\n\u001b[1;32m    959\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    960\u001b[0m     input_steps \u001b[39m=\u001b[39m x[\u001b[39m\"\u001b[39m\u001b[39mn_steps\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Sync/informatique/programmation/python/python_par_jupyter/.venv/lib/python3.10/site-packages/nengo_dl/simulator.py:1929\u001b[0m, in \u001b[0;36mSimulator._check_data\u001b[0;34m(self, data, batch_size, n_steps, nodes)\u001b[0m\n\u001b[1;32m   1927\u001b[0m \u001b[39m# generic shape checks\u001b[39;00m\n\u001b[1;32m   1928\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m-> 1929\u001b[0m     \u001b[39mraise\u001b[39;00m ValidationError(\n\u001b[1;32m   1930\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshould have rank 3 (batch_size, n_steps, dimensions), found rank \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1931\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1932\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m data\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1933\u001b[0m     )\n\u001b[1;32m   1934\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mminibatch_size:\n\u001b[1;32m   1935\u001b[0m     \u001b[39mraise\u001b[39;00m ValidationError(\n\u001b[1;32m   1936\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch size of data (\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m) less than Simulator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1937\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`minibatch_size` (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mminibatch_size\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1938\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m data\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1939\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: node data: should have rank 3 (batch_size, n_steps, dimensions), found rank 4"
     ]
    }
   ],
   "source": [
    "sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "print(\n",
    "    \"Accuracy after training:\",\n",
    "    sim.evaluate(test_images, {out_p_filt: test_labels}, verbose=0)[\"loss\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sim.predict(test_images[:minibatch_size])\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(test_images[i, 0].reshape((28, 28)), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(tf.nn.softmax(data[out_p_filt][i]))\n",
    "    plt.legend([str(i) for i in range(10)], loc=\"upper left\")\n",
    "    plt.xlabel(\"timesteps\")\n",
    "    plt.ylabel(\"probability\")\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.nengo.ai/nengo-dl/examples/spiking-mnist.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
